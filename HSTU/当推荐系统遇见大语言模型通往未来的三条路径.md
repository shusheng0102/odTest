**©作者 | **张月鹏

**单位 | **得物

**研究方向 | **搜推算法

![079070b48423705d14a3069e91a9b14c.png](https://img-blog.csdnimg.cn/img_convert/079070b48423705d14a3069e91a9b14c.png)

### **前言**

自从大[语言模型](https://so.csdn.net/so/search?q=%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B&spm=1001.2101.3001.7020)爆火之后，大家对大语言模型（LLM）如何成功应用在推荐系统进行了不少尝试。个人一直觉得 LLM 在工业界推荐系统大部分情况还是离线应用，生成一些特征或者文本。

至于其直接用作召回和排序模块，个人对这块的观点一直是比较悲观的。出于两点考虑：1）推理耗时问题，大模型的推理耗时难以满足[推荐系统](https://so.csdn.net/so/search?q=%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F&spm=1001.2101.3001.7020)快速响应的体验要求；2）基于 ID 和用户行为推荐体系是语言模型难以学习的。

但是，Meta 的《Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations》第一次在大规模工业系统打败传统推荐系统的召回和排序模块。未来已来~

![a751af362170b1a9969b815ad0e34c6b.png](https://img-blog.csdnimg.cn/img_convert/a751af362170b1a9969b815ad0e34c6b.png)

首先想讲清楚几个概念和基本思路：

* 大模型，这里的大模型指的类似 Transfomer 结构，基于 attention 的方式的计算密集型大模型。因为如果只考虑参数量的话，推荐系统早已有千亿参数规模的大模型，只不过大部分参数都集中在 embedding 层，属于存贮密集型。
* 如上图，推荐系统借鉴大模型的经验主要有两条路在走。 **第 1 条路** ：直接利用 GPT 等大语言模型的能力，包括直接进行排序，获取用户和内容的特征等等。 **第 2 条路** ：借鉴大模型的结构，这里其实早就有 BERT4REC 等推荐模型借鉴了 Transfomer 的 self attention。然后借鉴大语言模型的思路在原有基础上变大。 **第 3 条路** ：直接将传统搜推模型变大。
* 大语言模型相比传统推荐系统的优势在于模型包含了广泛的开放世界知识和推理能力。

![363da75d861912bc8691295b4113f4a1.png](https://img-blog.csdnimg.cn/img_convert/363da75d861912bc8691295b4113f4a1.png)

### **大语言模型在推荐系统的应用（第1条路）**

A Survey on Large Language Models for Recommendation

*https://arxiv.org/abs/2305.19860*

文章主要的贡献还是总结和分类已有的在推荐系统利用大模型的方法。相关文献作者整理到 github：

*https://github.com/WLiK/LLM4Rec-Awesome-Papers/tree/main*

至于论文中提高的一些发现，基本都是老生常谈的发现，熟悉大语言模型的应该都有所了解。下面展开说下文中的两种分类体系。

![8cd173319f5b95c9ca695c729de13f99.png](https://img-blog.csdnimg.cn/img_convert/8cd173319f5b95c9ca695c729de13f99.png)

第一种分类体系，按照利用 LLM 方式的不同分为 3 类：第一类：利用 LLM 产出的 embedding 增强推荐系统；第二类：利用 LLM 产出的 token 增强推荐系统；第三类：直接使用 LLM 作为推荐系统的某个模块，比如召回和排序等。

![1cdb8908ae272d9279109aa427814fc0.png](https://img-blog.csdnimg.cn/img_convert/1cdb8908ae272d9279109aa427814fc0.png)

第二种体系。分为判别式和生成式，然后根据是否 tuning 和 tuning 方式进行细分，如上图。

How Can Recommender Systems Benefit from Large Language Models: A Survey

*https://arxiv.org/abs/2306.05817*

相比《A Survey on Large Language Models for Recommendation》这篇综述，本篇文章更加站在工业界应用的角度进行综述，个人觉得看综述看这篇更有参考意义。

![f96099c2ca8f388a24c19c952560503f.png](https://img-blog.csdnimg.cn/img_convert/f96099c2ca8f388a24c19c952560503f.png)

如上体，文章主要将文献按照应用在什么阶段（WHERE）和如何应用（HOW）进行分类。应用在推荐系统的阶段主要包括特征工程、特征编码、模型预估、用户交互、流程控制 5 个阶段。如何应用，主要在训练阶段是否微调，在推理阶段是否结合传统推荐系统，分为了 4 类。

文章分析了 LLM 在推荐系统应用的发展脉络。阶段 1：利用小规模语言模型的微调结合传统推荐模型；阶段 2：利用大语言模型不微调，直接作为推荐系统；阶段 3：将大语言模型进行微调或者结合传统推荐系统模型，或者既微调又结合传统推荐模型。

NoteLLM: A Retrievable Large Language Model for Note Recommendation

*https://arxiv.org/abs/2403.01744*

![18a2db262e5b68e6533c0b2e1440abd8.png](https://img-blog.csdnimg.cn/img_convert/18a2db262e5b68e6533c0b2e1440abd8.png)

#### 

主要思路就是使用 LLaMA 2 产出 note 的 embedding，并同时生成 note 的类别和标签。方式就是通过构造 prompt 模板来产出对应的 embedding 和生成类别和标签；

embedding 使用特征 token 最后一层的隐向量加一层 MLP 输出。然后和传统推荐系统一样通过 note 的共现构造正样本，负样本为 batch 内负采样，做一个对比学习的任务。

生成的类别和标签，和真实类别标签，做一个微调的 loss。

对比学习任务和生成任务是同时训练的。论文发现生成任务，能帮助提升 embedding 的质量，提升 i2i 召回效果。但是看数据提升并不大。

Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5)

*https://arxiv.org/abs/2203.13366*

#### ![4e4e1bc11c71f2c518006e25c3f6037e.png](https://img-blog.csdnimg.cn/img_convert/4e4e1bc11c71f2c518006e25c3f6037e.png)

如上图，文章主要就是把各种推荐方式，变成一种 token 生成的统一方式，来利用语言模型进行训练。

![b9025380f4449d610b459a49e7b62aed.png](https://img-blog.csdnimg.cn/img_convert/b9025380f4449d610b459a49e7b62aed.png)

文章引入了 Whole-word embedding 来让一个用户（例如 user_324）和一个 item（例如：item_68293）在分为 token 之后还会共享同一个 Whole-word embedding。这样能不引入大量 ID 特征的基础上，更好的保持个性化能力。

文章整体都是离线的调研实验类的，目前个人看不到工业实践可能性。

Prompt Learning for News Recommendation

*https://arxiv.org/abs/2304.05263*

文章数据直接利用 LLM 进行排序的方式。

![909d640c843b633edbbfc36403d82cc4.png](https://img-blog.csdnimg.cn/img_convert/909d640c843b633edbbfc36403d82cc4.png)

文章使用完形填空的方式，通过设计和训练 prompt（prompt learning），完成利用 LLM 预测 [MASK] 的词汇。具体词汇是一个二分类的词比如 yes/no、good/bad 对应是否点击。

文章不具备大规模工业界利用的能力。一方面全文都是离线训练和离线实验，无法应对线上 inference。一方面文章只在新闻推荐这个领域有效，因为其只利用了 LLM 的语言能力，在文本较少的视频或者商品推荐上必然效果一般。

M6-Rec: Generative Pretrained Language Models are Open-Ended Recommender Systems

*https://arxiv.org/abs/2205.08084*

主要思路是将推荐任务全部转化成文本。用户特征用文本描述，物料也用文本描述。然后，最后产出一个向量用作召回，或者产出一个向量叠加一个 softmax 用作排序，或者直接生成文本。

文章主要的点还是工程上如何做训练和推理优化，比如 late nteraction、option tuning（改进的 promot tuning） 等。

TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation

*https://arxiv.org/abs/2305.00447*

文章没干货，仅仅利用指令微调（微调方式是 LoRA）的方式，让开放的大语言模型更实验推荐场景，而且只有一种微调指令和方式。如下：

![ff89184e6196b8e1f7e2b2592e50176a.png](https://img-blog.csdnimg.cn/img_convert/ff89184e6196b8e1f7e2b2592e50176a.png)

CTRL: Connect Collaborative and Language Model for CTR Prediction

*https://arxiv.org/abs/2306.02841*

![957bcfbd942153028e421b783abd3d93.png](https://img-blog.csdnimg.cn/img_convert/957bcfbd942153028e421b783abd3d93.png)

#### 

文章通过两阶段的训练，来将 LLM 的知识注入排序模型，线上推理还是原来的排序模型，这样不会引入额外的耗时。第一阶段对齐传统模型和 LLM 对样本的 embedding 表示，而传统模型的样本会通过人工模板转换 LLM 对应的样本，具体 prompt 构造的示例，如下图。第二阶段就是传统的监督学习。

![9dd3ac12358817571cdb8632fab484c4.png](https://img-blog.csdnimg.cn/img_convert/9dd3ac12358817571cdb8632fab484c4.png)

关于这篇文章的结构，个人觉得还是不适合大规模工业系统的。因为大规模系统每天增量产生大量样本。这样两阶段的训练，每天增量的预训练和监督训练，会让模型无法专注学习到第二阶段的预估任务。而且数据量变大后，这样的知识注入效率应该很低。因为，高频的id特征不需要注入，低频的 id 特征这种结构也无法充分学习好先验知识。

Text Is All You Need: Learning Language Representations for Sequential Recommendation

*https://arxiv.org/abs/2305.13731*

这篇文章严格意义上不能算 LLM 在推荐系统的应用。应该算是 transfomer 结构和文本 token 在推荐系统的应用。把所有 item 用文本表示，那么用户的点击历史，多个 item 也可以表示成文本。在这样的文本上产生用户的 embedding 和 item 的 embedding，用于召回或者排序。

![d95fd79156137fa960af980fc52dc093.png](https://img-blog.csdnimg.cn/img_convert/d95fd79156137fa960af980fc52dc093.png)

如上图所示，将 item 或者 user 文本输入 transfomer 结构，然后用【CLS】作为 item 或者 user 的 embedding。embedding 的表示上额外引入了 token type embedding（表示属于特征域文本还是属于特征值文本，比如特征域文本是 brand，那么特征值文本可能是 Apple 等）和 item pos embedding（表示用户交互的第几个 item， 如果是要获取 item 的 embedding，那这里长度就对应为 1）。

![271eb08c398a20e01226decda8ef5da8.png](https://img-blog.csdnimg.cn/img_convert/271eb08c398a20e01226decda8ef5da8.png)

### **基于大模型的生成式推荐（第2条路）**

Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations

*https://arxiv.org/abs/2402.17152*

论文的代码已开源，见：

*https://github.com/facebookresearch/generative-recommenders*

关于论文的细节这里不再展开了，已经有更好的地方参考。见：如何评价 Meta 最新的推荐算法论文：统一的生成式推荐第一次打败了分层架构的深度推荐系统？，这个提问下，有不少大佬以及论文作者的一些回答。如果你要详读这篇论文。除了论文本身外，建议参考这个提问。

如果不确认是否要详读这篇论文。这里给出几点参考：

* 论文相对来说是晦涩难懂的，一方面里面的一些符号和定义需要结合正文和附录来回翻看。另外一方面论文里面涉及较多的工程细节，算法同学看起来相对吃力。
* 工程上看，论文把模型做大了，最大的模型算力接近 GPT3。但是不确定 Meta 做到线上的有多大。论文介绍了如何在训练和预估的时候节省算力。训练阶段包括随机长度和样本采样逻辑的优化，inference 阶段主要是批量预估的逻辑。
* 算法上看，论文虽然是借鉴 transfomer 模型的，但是与 BERT4REC 和SASRec 等有较大区别。第 1 点：经典的序列建模通常只建模一条同质的序列，比如，基于点击的内容序列建模。而本文利用的序列包含多个异质序列，比如融合点击内容和关注作者等等。第 2 点：以往的序列生成式建模无法与 target 进行交互，这里实现了和 target 的交互。第 3 点：模型结构借鉴了推荐排序模型常用的特征交叉思路，来优化 attention 过程。
* 整体来看，这个工作值得 follow。但是一方面需要强大的工程团队支持优化，一方面这里的模型还是无法支持传统搜推模型的大量人工特征，效果打平的难度不小。可能只有大厂核心业务有人力和资源支持这个工作的 follow。

![c9933493deed66d21a90d0aba1c4ab47.png](https://img-blog.csdnimg.cn/img_convert/c9933493deed66d21a90d0aba1c4ab47.png)

### **传统搜推模型变大（第3条路）**

Wukong: Towards a Scaling Law for Large-Scale Recommendation

*https://arxiv.org/abs/2403.02545*

论文的主要思路就是把模型变大，展现出可扩展的 Scaling Law。这里的变大指的非 embedding 层的变大。

![b57fd74ca4c5999f8b5956697a609f48.png](https://img-blog.csdnimg.cn/img_convert/b57fd74ca4c5999f8b5956697a609f48.png)

论文主要的模型结构如上：第一步：获取 embedding，和传统推荐思路类似；第二步：堆叠多层 wukong layer，这里的 wukong layer 至于基于 FM 和 MLP 构建；第三步：最后接一个 MLP 作为输出。

论文只是在 meta 的数据集上做了离线实验，没有线上的 AB 实验。考虑这里主要是线上 inference 的耗时问题。

![3dcf2aa92d721d015b262af918d1daf5.png](https://img-blog.csdnimg.cn/img_convert/3dcf2aa92d721d015b262af918d1daf5.png)

### **总结**

Meta的《Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations》，大模型直接作排序是十分有前景的方向，但是需要工程和算力的不断进化，来变为后续的标准范式。

像小红书《NoteLLM: A Retrievable Large Language Model for Note Recommendation》和华为《CTRL: Connect Collaborative and Language Model for CTR Prediction》一样，离线使用 LLM 产出一些特征或者增强数据，还是比较简单实用的方向
